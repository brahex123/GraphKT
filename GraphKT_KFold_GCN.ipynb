{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXYhnccdXdKY",
        "outputId": "9ac33cb2-e29c-408e-979f-cecf8f0604fc",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.13.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2026.1.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.15.0)\n",
            "Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11hq0iBhW7Kb",
        "outputId": "6f97f318-b2f0-4248-dd1c-ff0f82bfdba9",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "PyTorch version: 2.9.0+cu126\n",
            "Configuration loaded successfully\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Cell 1: Imports and Configuration\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import HeteroData\n",
        "from torch_geometric.nn import GCNConv, RGCNConv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration\n",
        "class Config:\n",
        "    # Data\n",
        "    SEED = 42\n",
        "    TEST_STUDENT_RATIO = 0.15      # 15% students for TEST (Split B)\n",
        "    VAL_STUDENT_RATIO = 0.15       # 15% of TRAIN students for VAL\n",
        "\n",
        "    # Model (start small)\n",
        "    EMBED_DIM = 32\n",
        "    HIDDEN_DIM = 64\n",
        "    NUM_GNN_LAYERS = 2\n",
        "    DROPOUT = 0.2\n",
        "\n",
        "    # Training\n",
        "    BATCH_SIZE = 64\n",
        "    LEARNING_RATE = 1e-3\n",
        "    WEIGHT_DECAY = 0.01\n",
        "    EPOCHS = 100\n",
        "    PATIENCE = 10\n",
        "    GRAD_CLIP = 1.0\n",
        "\n",
        "    # Mastery\n",
        "    MASTERY_INIT = 0.5\n",
        "    MASTERY_ALPHA = 1.0  # Bayesian smoothing parameter\n",
        "\n",
        "    # Device\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "np.random.seed(config.SEED)\n",
        "torch.manual_seed(config.SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(config.SEED)\n",
        "\n",
        "print(f\"Device: {config.DEVICE}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Configuration loaded successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g3UDZq76XawB",
        "outputId": "bab0676c-b4b5-4d71-e86a-a41dcd6af303",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "DATASET OVERVIEW\n",
            "============================================================\n",
            "Shape: (790142, 19)\n",
            "\n",
            "Columns (19):\n",
            "   1. Row\n",
            "   2. Anon Student Id\n",
            "   3. Problem Hierarchy\n",
            "   4. Problem Name\n",
            "   5. Problem View\n",
            "   6. Step Name\n",
            "   7. Step Start Time\n",
            "   8. First Transaction Time\n",
            "   9. Correct Transaction Time\n",
            "  10. Step End Time\n",
            "  11. Step Duration (sec)\n",
            "  12. Correct Step Duration (sec)\n",
            "  13. Error Step Duration (sec)\n",
            "  14. Correct First Attempt\n",
            "  15. Incorrects\n",
            "  16. Hints\n",
            "  17. Corrects\n",
            "  18. KC(Default)\n",
            "  19. Opportunity(Default)\n",
            "\n",
            "============================================================\n",
            "FIRST 3 ROWS\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   Row Anon Student Id            Problem Hierarchy Problem Name  \\\n",
              "0    1      0BrbPbwCMz  Unit ES_04, Section ES_04-1    EG4-FIXED   \n",
              "1    2      0BrbPbwCMz  Unit ES_04, Section ES_04-1    EG4-FIXED   \n",
              "2    3      0BrbPbwCMz  Unit ES_04, Section ES_04-1         EG40   \n",
              "\n",
              "   Problem View    Step Name        Step Start Time First Transaction Time  \\\n",
              "0             1  3(x+2) = 15  2005-09-09 12:24:35.0  2005-09-09 12:24:49.0   \n",
              "1             1      x+2 = 5  2005-09-09 12:25:15.0  2005-09-09 12:25:31.0   \n",
              "2             1    2-8y = -4  2005-09-09 12:25:36.0  2005-09-09 12:25:43.0   \n",
              "\n",
              "  Correct Transaction Time          Step End Time  Step Duration (sec)  \\\n",
              "0    2005-09-09 12:25:15.0  2005-09-09 12:25:15.0                 40.0   \n",
              "1    2005-09-09 12:25:31.0  2005-09-09 12:25:31.0                 16.0   \n",
              "2    2005-09-09 12:26:12.0  2005-09-09 12:26:12.0                 36.0   \n",
              "\n",
              "   Correct Step Duration (sec)  Error Step Duration (sec)  \\\n",
              "0                          NaN                       40.0   \n",
              "1                         16.0                        NaN   \n",
              "2                          NaN                       36.0   \n",
              "\n",
              "   Correct First Attempt  Incorrects  Hints  Corrects  \\\n",
              "0                      0           2      3         1   \n",
              "1                      1           0      0         1   \n",
              "2                      0           2      3         1   \n",
              "\n",
              "                                         KC(Default) Opportunity(Default)  \n",
              "0  [SkillRule: Eliminate Parens; {CLT nested; CLT...                    1  \n",
              "1  [SkillRule: Remove constant; {ax+b=c, positive...                 1~~1  \n",
              "2  [SkillRule: Remove constant; {ax+b=c, positive...                    2  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-01313b02-7549-44ce-80b9-f6d7203a677d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Row</th>\n",
              "      <th>Anon Student Id</th>\n",
              "      <th>Problem Hierarchy</th>\n",
              "      <th>Problem Name</th>\n",
              "      <th>Problem View</th>\n",
              "      <th>Step Name</th>\n",
              "      <th>Step Start Time</th>\n",
              "      <th>First Transaction Time</th>\n",
              "      <th>Correct Transaction Time</th>\n",
              "      <th>Step End Time</th>\n",
              "      <th>Step Duration (sec)</th>\n",
              "      <th>Correct Step Duration (sec)</th>\n",
              "      <th>Error Step Duration (sec)</th>\n",
              "      <th>Correct First Attempt</th>\n",
              "      <th>Incorrects</th>\n",
              "      <th>Hints</th>\n",
              "      <th>Corrects</th>\n",
              "      <th>KC(Default)</th>\n",
              "      <th>Opportunity(Default)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0BrbPbwCMz</td>\n",
              "      <td>Unit ES_04, Section ES_04-1</td>\n",
              "      <td>EG4-FIXED</td>\n",
              "      <td>1</td>\n",
              "      <td>3(x+2) = 15</td>\n",
              "      <td>2005-09-09 12:24:35.0</td>\n",
              "      <td>2005-09-09 12:24:49.0</td>\n",
              "      <td>2005-09-09 12:25:15.0</td>\n",
              "      <td>2005-09-09 12:25:15.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>[SkillRule: Eliminate Parens; {CLT nested; CLT...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0BrbPbwCMz</td>\n",
              "      <td>Unit ES_04, Section ES_04-1</td>\n",
              "      <td>EG4-FIXED</td>\n",
              "      <td>1</td>\n",
              "      <td>x+2 = 5</td>\n",
              "      <td>2005-09-09 12:25:15.0</td>\n",
              "      <td>2005-09-09 12:25:31.0</td>\n",
              "      <td>2005-09-09 12:25:31.0</td>\n",
              "      <td>2005-09-09 12:25:31.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[SkillRule: Remove constant; {ax+b=c, positive...</td>\n",
              "      <td>1~~1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0BrbPbwCMz</td>\n",
              "      <td>Unit ES_04, Section ES_04-1</td>\n",
              "      <td>EG40</td>\n",
              "      <td>1</td>\n",
              "      <td>2-8y = -4</td>\n",
              "      <td>2005-09-09 12:25:36.0</td>\n",
              "      <td>2005-09-09 12:25:43.0</td>\n",
              "      <td>2005-09-09 12:26:12.0</td>\n",
              "      <td>2005-09-09 12:26:12.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>[SkillRule: Remove constant; {ax+b=c, positive...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-01313b02-7549-44ce-80b9-f6d7203a677d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-01313b02-7549-44ce-80b9-f6d7203a677d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-01313b02-7549-44ce-80b9-f6d7203a677d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(df_raw['Correct First Attempt']\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Row\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Anon Student Id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"0BrbPbwCMz\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Problem Hierarchy\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Unit ES_04, Section ES_04-1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Problem Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"EG40\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Problem View\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Step Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"3(x+2) = 15\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Step Start Time\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"2005-09-09 12:24:35.0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"First Transaction Time\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"2005-09-09 12:24:49.0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Correct Transaction Time\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"2005-09-09 12:25:15.0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Step End Time\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"2005-09-09 12:25:15.0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Step Duration (sec)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.858201014657274,\n        \"min\": 16.0,\n        \"max\": 40.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          40.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Correct Step Duration (sec)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 16.0,\n        \"max\": 16.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          16.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Error Step Duration (sec)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.8284271247461903,\n        \"min\": 36.0,\n        \"max\": 40.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          36.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Correct First Attempt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Incorrects\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Hints\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Corrects\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"KC(Default)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"[SkillRule: Eliminate Parens; {CLT nested; CLT nested, parens; Distribute Mult right; Distribute Mult left; (+/-x +/-a)/b=c, mult; (+/-x +/-a)*b=c, div; [var expr]/[const expr] = [const expr], multiply; Distribute Division left; Distribute Division right; Distribute both mult left; Distribute both mult right; Distribute both divide left; Distribute both divide right; Distribute subex}]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Opportunity(Default)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "DATA TYPES\n",
            "============================================================\n",
            "Row                              int64\n",
            "Anon Student Id                 object\n",
            "Problem Hierarchy               object\n",
            "Problem Name                    object\n",
            "Problem View                     int64\n",
            "Step Name                       object\n",
            "Step Start Time                 object\n",
            "First Transaction Time          object\n",
            "Correct Transaction Time        object\n",
            "Step End Time                   object\n",
            "Step Duration (sec)            float64\n",
            "Correct Step Duration (sec)    float64\n",
            "Error Step Duration (sec)      float64\n",
            "Correct First Attempt            int64\n",
            "Incorrects                       int64\n",
            "Hints                            int64\n",
            "Corrects                         int64\n",
            "KC(Default)                     object\n",
            "Opportunity(Default)            object\n",
            "dtype: object\n",
            "\n",
            "============================================================\n",
            "MISSING VALUES\n",
            "============================================================\n",
            "                             Missing  Percent\n",
            "Step Start Time                  870     0.11\n",
            "Correct Transaction Time       25406     3.22\n",
            "Step Duration (sec)              870     0.11\n",
            "Correct Step Duration (sec)   185398    23.46\n",
            "Error Step Duration (sec)     605614    76.65\n",
            "KC(Default)                   197833    25.04\n",
            "Opportunity(Default)          197834    25.04\n",
            "\n",
            "============================================================\n",
            "KEY STATISTICS\n",
            "============================================================\n",
            "Total interactions: 790,142\n",
            "Unique students: 534\n",
            "Unique problems (questions): 1,084\n",
            "Unique steps: 205,481\n",
            "Unique KC(Default): 437\n",
            "KC(Default) missing: 197,833 (25.04%)\n",
            "\n",
            "============================================================\n",
            "TARGET DISTRIBUTION (Correct First Attempt)\n",
            "============================================================\n",
            "Correct First Attempt\n",
            "1    0.766\n",
            "0    0.234\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Cell 2: Load and Explore Dataset\n",
        "# =============================================================================\n",
        "\n",
        "# Load the Algebra 2005-2006 dataset\n",
        "DATA_PATH = \"algebra_2005_2006_train.txt\"\n",
        "\n",
        "# Load with tab separator (standard format for this dataset)\n",
        "df_raw = pd.read_csv(DATA_PATH, sep='\\t', low_memory=False)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"DATASET OVERVIEW\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Shape: {df_raw.shape}\")\n",
        "print(f\"\\nColumns ({len(df_raw.columns)}):\")\n",
        "for i, col in enumerate(df_raw.columns):\n",
        "    print(f\"  {i+1:2d}. {col}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FIRST 3 ROWS\")\n",
        "print(\"=\" * 60)\n",
        "display(df_raw.head(3))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"DATA TYPES\")\n",
        "print(\"=\" * 60)\n",
        "print(df_raw.dtypes)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"MISSING VALUES\")\n",
        "print(\"=\" * 60)\n",
        "missing = df_raw.isnull().sum()\n",
        "missing_pct = (missing / len(df_raw) * 100).round(2)\n",
        "missing_df = pd.DataFrame({'Missing': missing, 'Percent': missing_pct})\n",
        "print(missing_df[missing_df['Missing'] > 0])\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"KEY STATISTICS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total interactions: {len(df_raw):,}\")\n",
        "print(f\"Unique students: {df_raw['Anon Student Id'].nunique():,}\")\n",
        "print(f\"Unique problems (questions): {df_raw['Problem Name'].nunique():,}\")\n",
        "print(f\"Unique steps: {df_raw[['Problem Name', 'Step Name']].drop_duplicates().shape[0]:,}\")\n",
        "print(f\"Unique KC(Default): {df_raw['KC(Default)'].nunique():,}\")\n",
        "print(f\"KC(Default) missing: {df_raw['KC(Default)'].isnull().sum():,} ({df_raw['KC(Default)'].isnull().mean()*100:.2f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TARGET DISTRIBUTION (Correct First Attempt)\")\n",
        "print(\"=\" * 60)\n",
        "print(df_raw['Correct First Attempt'].value_counts(normalize=True).round(4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4X0IrVL6X5Du",
        "outputId": "bd8e5c6f-985c-4c8c-e3f9-2c6434e1751d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "STEP 1: Drop rows with missing critical fields (NOT KC)\n",
            "============================================================\n",
            "Dropped 0 rows (0.00%)\n",
            "Remaining: 790,142 rows\n",
            "\n",
            "============================================================\n",
            "STEP 2: Handle missing KC(Default)\n",
            "============================================================\n",
            "Missing KC(Default): 197,833 (25.04%)\n",
            "Filled with 'UNKNOWN_KC' token\n",
            "\n",
            "============================================================\n",
            "STEP 3: Create canonical identifiers\n",
            "============================================================\n",
            "Unique students: 534\n",
            "Unique questions: 1,084\n",
            "Unique steps: 205,481\n",
            "Unique KCs (including UNKNOWN): 438\n",
            "\n",
            "============================================================\n",
            "STEP 4: Parse timestamps and create temporal ordering\n",
            "============================================================\n",
            "Rows with missing timestamp after fallback: 0\n",
            "Final dataset size: 790,142 rows\n",
            "\n",
            "============================================================\n",
            "STEP 5: Process behavioral features\n",
            "============================================================\n",
            "Median duration: 11.00 sec\n",
            "Log duration range: [0.00, 7.90]\n",
            "\n",
            "============================================================\n",
            "STEP 6: Final dataset summary\n",
            "============================================================\n",
            "Total interactions: 790,142\n",
            "Unique students: 534\n",
            "Unique questions: 1,084\n",
            "Unique steps: 205,481\n",
            "Unique KCs: 438\n",
            "  - Real KCs: 592,309 interactions\n",
            "  - UNKNOWN_KC: 197,833 interactions\n",
            "\n",
            "Target distribution:\n",
            "correct\n",
            "1    0.766\n",
            "0    0.234\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "============================================================\n",
            "STEP 7: Verify temporal ordering\n",
            "============================================================\n",
            "Sample student '02ZjVTxC34' first 5 interactions:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   time_idx           timestamp question_id  \\\n",
              "0         0 2005-09-06 13:00:23  LDEMO_WKST   \n",
              "1         1 2005-09-06 13:00:44  LDEMO_WKST   \n",
              "2         2 2005-09-06 13:01:12  LDEMO_WKST   \n",
              "3         3 2005-09-06 13:01:46  LDEMO_WKST   \n",
              "4         4 2005-09-06 13:02:27  LDEMO_WKST   \n",
              "\n",
              "                                        kc_id  correct  \n",
              "0                                  UNKNOWN_KC        1  \n",
              "1                           Identifying units        1  \n",
              "2                                  UNKNOWN_KC        1  \n",
              "3                           Identifying units        1  \n",
              "4  Entering a given~~Convert unit, multiplier        1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-61fc83dc-8e0f-47ca-9a42-30aca209b51a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time_idx</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>question_id</th>\n",
              "      <th>kc_id</th>\n",
              "      <th>correct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2005-09-06 13:00:23</td>\n",
              "      <td>LDEMO_WKST</td>\n",
              "      <td>UNKNOWN_KC</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2005-09-06 13:00:44</td>\n",
              "      <td>LDEMO_WKST</td>\n",
              "      <td>Identifying units</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2005-09-06 13:01:12</td>\n",
              "      <td>LDEMO_WKST</td>\n",
              "      <td>UNKNOWN_KC</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2005-09-06 13:01:46</td>\n",
              "      <td>LDEMO_WKST</td>\n",
              "      <td>Identifying units</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2005-09-06 13:02:27</td>\n",
              "      <td>LDEMO_WKST</td>\n",
              "      <td>Entering a given~~Convert unit, multiplier</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-61fc83dc-8e0f-47ca-9a42-30aca209b51a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-61fc83dc-8e0f-47ca-9a42-30aca209b51a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-61fc83dc-8e0f-47ca-9a42-30aca209b51a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_d48ae988-b15d-41a4-9a48-c5d29ce69e3d\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('sample_seq')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_d48ae988-b15d-41a4-9a48-c5d29ce69e3d button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('sample_seq');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "sample_seq",
              "summary": "{\n  \"name\": \"sample_seq\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"time_idx\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1,\n          4,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2005-09-06 13:00:23\",\n        \"max\": \"2005-09-06 13:02:27\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2005-09-06 13:00:44\",\n          \"2005-09-06 13:02:27\",\n          \"2005-09-06 13:01:12\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"LDEMO_WKST\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"kc_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"UNKNOWN_KC\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"correct\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Cell 3 (Corrected): Data Cleaning - Keep Missing KC as UNKNOWN\n",
        "# =============================================================================\n",
        "\n",
        "# Start with a copy\n",
        "df = df_raw.copy()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 1: Drop rows with missing critical fields (NOT KC)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Critical fields: student, step, target (NOT KC - we'll handle separately)\n",
        "critical_cols = ['Anon Student Id', 'Problem Name', 'Step Name', 'Correct First Attempt']\n",
        "before_drop = len(df)\n",
        "df = df.dropna(subset=critical_cols)\n",
        "after_drop = len(df)\n",
        "print(f\"Dropped {before_drop - after_drop:,} rows ({(before_drop - after_drop)/before_drop*100:.2f}%)\")\n",
        "print(f\"Remaining: {after_drop:,} rows\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 2: Handle missing KC(Default)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "kc_missing_before = df['KC(Default)'].isnull().sum()\n",
        "print(f\"Missing KC(Default): {kc_missing_before:,} ({kc_missing_before/len(df)*100:.2f}%)\")\n",
        "\n",
        "# Fill missing KC with special token\n",
        "df['KC(Default)'] = df['KC(Default)'].fillna('UNKNOWN_KC')\n",
        "print(f\"Filled with 'UNKNOWN_KC' token\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 3: Create canonical identifiers\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Student ID\n",
        "df['student_id'] = df['Anon Student Id'].astype(str).str.strip()\n",
        "\n",
        "# Question ID (Problem Name)\n",
        "df['question_id'] = df['Problem Name'].astype(str).str.strip()\n",
        "\n",
        "# Step ID (Problem Name + Step Name)\n",
        "df['step_id'] = df['Problem Name'].astype(str).str.strip() + \"||\" + df['Step Name'].astype(str).str.strip()\n",
        "\n",
        "# KC ID (KC(Default) as composite string)\n",
        "df['kc_id'] = df['KC(Default)'].astype(str).str.strip()\n",
        "\n",
        "# Target\n",
        "df['correct'] = df['Correct First Attempt'].astype(int)\n",
        "\n",
        "print(f\"Unique students: {df['student_id'].nunique():,}\")\n",
        "print(f\"Unique questions: {df['question_id'].nunique():,}\")\n",
        "print(f\"Unique steps: {df['step_id'].nunique():,}\")\n",
        "print(f\"Unique KCs (including UNKNOWN): {df['kc_id'].nunique():,}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 4: Parse timestamps and create temporal ordering\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Parse First Transaction Time (primary timestamp)\n",
        "df['timestamp'] = pd.to_datetime(df['First Transaction Time'], errors='coerce')\n",
        "\n",
        "# Fallback to Step Start Time\n",
        "mask_missing_ts = df['timestamp'].isnull()\n",
        "df.loc[mask_missing_ts, 'timestamp'] = pd.to_datetime(\n",
        "    df.loc[mask_missing_ts, 'Step Start Time'], errors='coerce'\n",
        ")\n",
        "\n",
        "# Check remaining missing timestamps\n",
        "ts_missing = df['timestamp'].isnull().sum()\n",
        "print(f\"Rows with missing timestamp after fallback: {ts_missing}\")\n",
        "\n",
        "if ts_missing > 0:\n",
        "    # Drop only these (should be minimal)\n",
        "    df = df.dropna(subset=['timestamp'])\n",
        "    print(f\"Dropped {ts_missing} rows with no valid timestamp\")\n",
        "\n",
        "# Sort by student and timestamp\n",
        "df = df.sort_values(['student_id', 'timestamp']).reset_index(drop=True)\n",
        "\n",
        "# Create time index within each student\n",
        "df['time_idx'] = df.groupby('student_id').cumcount()\n",
        "\n",
        "print(f\"Final dataset size: {len(df):,} rows\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 5: Process behavioral features\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Fill missing durations with median\n",
        "duration_col = 'Step Duration (sec)'\n",
        "median_duration = df[duration_col].median()\n",
        "df[duration_col] = df[duration_col].fillna(median_duration)\n",
        "\n",
        "# Log transform duration\n",
        "df['log_duration'] = np.log1p(df[duration_col].clip(lower=0))\n",
        "\n",
        "# Clip extreme values\n",
        "df['Incorrects'] = df['Incorrects'].clip(upper=10)\n",
        "df['Hints'] = df['Hints'].clip(upper=10)\n",
        "\n",
        "print(f\"Median duration: {median_duration:.2f} sec\")\n",
        "print(f\"Log duration range: [{df['log_duration'].min():.2f}, {df['log_duration'].max():.2f}]\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 6: Final dataset summary\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"Total interactions: {len(df):,}\")\n",
        "print(f\"Unique students: {df['student_id'].nunique():,}\")\n",
        "print(f\"Unique questions: {df['question_id'].nunique():,}\")\n",
        "print(f\"Unique steps: {df['step_id'].nunique():,}\")\n",
        "print(f\"Unique KCs: {df['kc_id'].nunique():,}\")\n",
        "print(f\"  - Real KCs: {(df['kc_id'] != 'UNKNOWN_KC').sum():,} interactions\")\n",
        "print(f\"  - UNKNOWN_KC: {(df['kc_id'] == 'UNKNOWN_KC').sum():,} interactions\")\n",
        "\n",
        "print(f\"\\nTarget distribution:\")\n",
        "print(df['correct'].value_counts(normalize=True).round(4))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 7: Verify temporal ordering\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "sample_student = df['student_id'].iloc[0]\n",
        "sample_seq = df[df['student_id'] == sample_student][['time_idx', 'timestamp', 'question_id', 'kc_id', 'correct']].head(5)\n",
        "print(f\"Sample student '{sample_student}' first 5 interactions:\")\n",
        "display(sample_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LvLbzcxFo-pm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "cc211fd2-2b2b-4642-bd12-9530843cecd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "STEP 1: SEPARATE TEST STUDENTS (HELD OUT ENTIRELY)\n",
            "============================================================\n",
            "Total students: 534\n",
            "\n",
            "TEST set (held out):\n",
            "  Students: 81 (15.2%)\n",
            "  Interactions: 111,057\n",
            "\n",
            "Non-test (enters K-Fold CV):\n",
            "  Students: 453 (84.8%)\n",
            "  Interactions: 679,085\n",
            "\n",
            "============================================================\n",
            "STEP 2: DEFINE 5-FOLD STUDENT-LEVEL SPLITS\n",
            "============================================================\n",
            "\n",
            "Fold 1:\n",
            "  TRAIN: 362 students\n",
            "  VAL:   91 students\n",
            "\n",
            "Fold 2:\n",
            "  TRAIN: 362 students\n",
            "  VAL:   91 students\n",
            "\n",
            "Fold 3:\n",
            "  TRAIN: 362 students\n",
            "  VAL:   91 students\n",
            "\n",
            "Fold 4:\n",
            "  TRAIN: 363 students\n",
            "  VAL:   90 students\n",
            "\n",
            "Fold 5:\n",
            "  TRAIN: 363 students\n",
            "  VAL:   90 students\n",
            "\n",
            "✓ All folds verified: no student overlap, no test leakage\n",
            "✓ TEST set (81 students) completely isolated\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Cell 4: Separate TEST Set + 5-Fold Student-Level CV Setup\n",
        "# =============================================================================\n",
        "\n",
        "import time\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from collections import defaultdict\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 1: SEPARATE TEST STUDENTS (HELD OUT ENTIRELY)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "all_students = df['student_id'].unique()\n",
        "n_students = len(all_students)\n",
        "print(f\"Total students: {n_students}\")\n",
        "\n",
        "# Hold out 15% of students as TEST - NEVER touched during CV\n",
        "non_test_students, test_students = train_test_split(\n",
        "    all_students,\n",
        "    test_size=config.TEST_STUDENT_RATIO,\n",
        "    random_state=config.SEED\n",
        ")\n",
        "\n",
        "df_test_final = df[df['student_id'].isin(test_students)].copy()\n",
        "df_non_test = df[df['student_id'].isin(non_test_students)].copy()\n",
        "\n",
        "print(f\"\\nTEST set (held out):\")\n",
        "print(f\"  Students: {len(test_students)} ({len(test_students)/n_students*100:.1f}%)\")\n",
        "print(f\"  Interactions: {len(df_test_final):,}\")\n",
        "\n",
        "print(f\"\\nNon-test (enters K-Fold CV):\")\n",
        "print(f\"  Students: {len(non_test_students)} ({len(non_test_students)/n_students*100:.1f}%)\")\n",
        "print(f\"  Interactions: {len(df_non_test):,}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 2: DEFINE 5-FOLD STUDENT-LEVEL SPLITS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=config.SEED)\n",
        "\n",
        "fold_assignments = {}\n",
        "for fold_idx, (train_indices, val_indices) in enumerate(kf.split(non_test_students)):\n",
        "    train_studs = non_test_students[train_indices]\n",
        "    val_studs = non_test_students[val_indices]\n",
        "    fold_assignments[fold_idx] = {\n",
        "        'train_students': train_studs,\n",
        "        'val_students': val_studs\n",
        "    }\n",
        "    print(f\"\\nFold {fold_idx+1}:\")\n",
        "    print(f\"  TRAIN: {len(train_studs)} students\")\n",
        "    print(f\"  VAL:   {len(val_studs)} students\")\n",
        "\n",
        "    # Verify no overlap\n",
        "    overlap = set(train_studs) & set(val_studs)\n",
        "    assert len(overlap) == 0, f\"LEAK in fold {fold_idx+1}!\"\n",
        "\n",
        "    # Verify no test leakage\n",
        "    test_leak = set(train_studs) & set(test_students)\n",
        "    assert len(test_leak) == 0, f\"TEST LEAK in fold {fold_idx+1}!\"\n",
        "\n",
        "print(\"\\n✓ All folds verified: no student overlap, no test leakage\")\n",
        "print(f\"✓ TEST set ({len(test_students)} students) completely isolated\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fiDbwzUto-pm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "43ce4633-f785-4545-c7b6-3bbdcdcb8f73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ All pipeline functions defined\n",
            "  - build_entity_mappings()\n",
            "  - build_graph()\n",
            "  - compute_node_features()\n",
            "  - GCNPure model\n",
            "  - KTDatasetPure dataset\n",
            "  - train_epoch() / evaluate()\n",
            "  - run_single_fold()\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Cell 5: Complete Pipeline Functions (Reusable Per Fold)\n",
        "# =============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import HeteroData\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "\n",
        "NUM_FEATURES = 5\n",
        "\n",
        "# ============================================================\n",
        "# FUNCTION 1: Build Entity Mappings\n",
        "# ============================================================\n",
        "def build_entity_mappings(df_train):\n",
        "    \"\"\"\n",
        "    Build entity-to-index mappings from TRAIN data only.\n",
        "    Adds UNK tokens at the end of each entity type.\n",
        "\n",
        "    Returns:\n",
        "        mappings dict, entity_counts dict, UNK indices dict\n",
        "    \"\"\"\n",
        "    train_students = sorted(df_train['student_id'].unique())\n",
        "    train_questions = sorted(df_train['question_id'].unique())\n",
        "    train_steps = sorted(df_train['step_id'].unique())\n",
        "    train_kcs = sorted(df_train['kc_id'].unique())\n",
        "\n",
        "    stu2idx = {s: i for i, s in enumerate(train_students)}\n",
        "    q2idx = {q: i for i, q in enumerate(train_questions)}\n",
        "    t2idx = {t: i for i, t in enumerate(train_steps)}\n",
        "    c2idx = {c: i for i, c in enumerate(train_kcs)}\n",
        "\n",
        "    unk_indices = {\n",
        "        'student': len(train_students),\n",
        "        'question': len(train_questions),\n",
        "        'step': len(train_steps),\n",
        "        'kc': len(train_kcs)\n",
        "    }\n",
        "\n",
        "    entity_counts = {\n",
        "        'num_students': len(train_students) + 1,\n",
        "        'num_questions': len(train_questions) + 1,\n",
        "        'num_steps': len(train_steps) + 1,\n",
        "        'num_kcs': len(train_kcs) + 1,\n",
        "    }\n",
        "\n",
        "    mappings = {\n",
        "        'stu2idx': stu2idx, 'q2idx': q2idx,\n",
        "        't2idx': t2idx, 'c2idx': c2idx\n",
        "    }\n",
        "\n",
        "    return mappings, entity_counts, unk_indices\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# FUNCTION 2: Build Heterogeneous Graph\n",
        "# ============================================================\n",
        "def build_graph(df_train, mappings, entity_counts, unk_indices):\n",
        "    \"\"\"\n",
        "    Build PyG HeteroData graph from TRAIN data only.\n",
        "\n",
        "    Returns:\n",
        "        HeteroData with node counts and edge indices\n",
        "    \"\"\"\n",
        "    stu2idx = mappings['stu2idx']\n",
        "    q2idx = mappings['q2idx']\n",
        "    t2idx = mappings['t2idx']\n",
        "    c2idx = mappings['c2idx']\n",
        "\n",
        "    data = HeteroData()\n",
        "\n",
        "    # Node counts (including UNK)\n",
        "    data['student'].num_nodes = entity_counts['num_students']\n",
        "    data['question'].num_nodes = entity_counts['num_questions']\n",
        "    data['step'].num_nodes = entity_counts['num_steps']\n",
        "    data['kc'].num_nodes = entity_counts['num_kcs']\n",
        "\n",
        "    # Q-T edges\n",
        "    qt_pairs = df_train[['question_id', 'step_id']].drop_duplicates()\n",
        "    q_idx_list = [q2idx[r['question_id']] for _, r in qt_pairs.iterrows()]\n",
        "    t_idx_list = [t2idx[r['step_id']] for _, r in qt_pairs.iterrows()]\n",
        "\n",
        "    data['question', 'contains', 'step'].edge_index = torch.tensor([q_idx_list, t_idx_list], dtype=torch.long)\n",
        "    data['step', 'belongs_to', 'question'].edge_index = torch.tensor([t_idx_list, q_idx_list], dtype=torch.long)\n",
        "\n",
        "    # T-C edges\n",
        "    tc_pairs = df_train[['step_id', 'kc_id']].drop_duplicates()\n",
        "    t_idx_list2 = [t2idx[r['step_id']] for _, r in tc_pairs.iterrows()]\n",
        "    c_idx_list = [c2idx[r['kc_id']] for _, r in tc_pairs.iterrows()]\n",
        "\n",
        "    data['step', 'requires', 'kc'].edge_index = torch.tensor([t_idx_list2, c_idx_list], dtype=torch.long)\n",
        "    data['kc', 'required_by', 'step'].edge_index = torch.tensor([c_idx_list, t_idx_list2], dtype=torch.long)\n",
        "\n",
        "    # S-Q edges\n",
        "    sq_pairs = df_train[['student_id', 'question_id']].drop_duplicates()\n",
        "    s_idx_list = [stu2idx[r['student_id']] for _, r in sq_pairs.iterrows()]\n",
        "    q_idx_list2 = [q2idx[r['question_id']] for _, r in sq_pairs.iterrows()]\n",
        "\n",
        "    data['student', 'attempted', 'question'].edge_index = torch.tensor([s_idx_list, q_idx_list2], dtype=torch.long)\n",
        "    data['question', 'attempted_by', 'student'].edge_index = torch.tensor([q_idx_list2, s_idx_list], dtype=torch.long)\n",
        "\n",
        "    total_edges = sum(\n",
        "        data[et].edge_index.shape[1]\n",
        "        for et in data.edge_types\n",
        "    )\n",
        "\n",
        "    return data, total_edges\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# FUNCTION 3: Compute Node Features\n",
        "# ============================================================\n",
        "def compute_node_features(df_train, mappings, entity_counts, unk_indices):\n",
        "    \"\"\"\n",
        "    Compute 5D statistical features for each entity from TRAIN data.\n",
        "    Returns dict of feature tensors per entity type.\n",
        "    \"\"\"\n",
        "    def compute_features_for_type(df, entity_col):\n",
        "        grouped = df.groupby(entity_col).agg({\n",
        "            'correct': ['count', 'mean'],\n",
        "            'log_duration': 'mean',\n",
        "            'Hints': 'mean',\n",
        "            'Incorrects': 'mean'\n",
        "        })\n",
        "        grouped.columns = ['freq', 'correct_rate', 'avg_log_dur', 'avg_hints', 'avg_incorrects']\n",
        "        grouped = grouped.reset_index()\n",
        "        grouped['difficulty'] = 1 - grouped['correct_rate']\n",
        "        grouped['log_freq'] = np.log1p(grouped['freq'])\n",
        "\n",
        "        features = {}\n",
        "        feat_cols = ['log_freq', 'difficulty', 'avg_log_dur', 'avg_hints', 'avg_incorrects']\n",
        "        for _, row in grouped.iterrows():\n",
        "            features[row[entity_col]] = row[feat_cols].values.astype(np.float32)\n",
        "        return features\n",
        "\n",
        "    def to_tensor(features_dict, idx_map, num_with_unk, unk_idx):\n",
        "        tensor = torch.zeros(num_with_unk, NUM_FEATURES, dtype=torch.float32)\n",
        "        all_feats = []\n",
        "        for entity_id, idx in idx_map.items():\n",
        "            if entity_id in features_dict:\n",
        "                tensor[idx] = torch.tensor(features_dict[entity_id])\n",
        "                all_feats.append(features_dict[entity_id])\n",
        "        if all_feats:\n",
        "            tensor[unk_idx] = torch.tensor(np.mean(all_feats, axis=0))\n",
        "        return tensor\n",
        "\n",
        "    def normalize(tensor):\n",
        "        mean = tensor.mean(dim=0, keepdim=True)\n",
        "        std = tensor.std(dim=0, keepdim=True) + 1e-8\n",
        "        return (tensor - mean) / std\n",
        "\n",
        "    # Compute for each type\n",
        "    stu_feats = compute_features_for_type(df_train, 'student_id')\n",
        "    q_feats = compute_features_for_type(df_train, 'question_id')\n",
        "    t_feats = compute_features_for_type(df_train, 'step_id')\n",
        "    c_feats = compute_features_for_type(df_train, 'kc_id')\n",
        "\n",
        "    # Convert to normalized tensors\n",
        "    feat_tensors = {\n",
        "        'student': normalize(to_tensor(stu_feats, mappings['stu2idx'], entity_counts['num_students'], unk_indices['student'])),\n",
        "        'question': normalize(to_tensor(q_feats, mappings['q2idx'], entity_counts['num_questions'], unk_indices['question'])),\n",
        "        'step': normalize(to_tensor(t_feats, mappings['t2idx'], entity_counts['num_steps'], unk_indices['step'])),\n",
        "        'kc': normalize(to_tensor(c_feats, mappings['c2idx'], entity_counts['num_kcs'], unk_indices['kc'])),\n",
        "    }\n",
        "\n",
        "    return feat_tensors\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# FUNCTION 4: Model Definition\n",
        "# ============================================================\n",
        "\n",
        "class NodeEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, embed_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, embed_dim),\n",
        "            nn.LayerNorm(embed_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(embed_dim, embed_dim),\n",
        "            nn.LayerNorm(embed_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "\n",
        "class GCNPure(nn.Module):\n",
        "    def __init__(self, num_students, num_questions, num_steps, num_kcs,\n",
        "                 feature_dim=5, embed_dim=32, hidden_dim=64,\n",
        "                 num_gnn_layers=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_students = num_students\n",
        "        self.num_questions = num_questions\n",
        "        self.num_steps = num_steps\n",
        "        self.num_kcs = num_kcs\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Offsets for unified indexing\n",
        "        self.student_offset = 0\n",
        "        self.question_offset = num_students\n",
        "        self.step_offset = num_students + num_questions\n",
        "        self.kc_offset = num_students + num_questions + num_steps\n",
        "\n",
        "        # Type-specific encoders\n",
        "        self.student_encoder = NodeEncoder(feature_dim, embed_dim, dropout)\n",
        "        self.question_encoder = NodeEncoder(feature_dim, embed_dim, dropout)\n",
        "        self.step_encoder = NodeEncoder(feature_dim, embed_dim, dropout)\n",
        "        self.kc_encoder = NodeEncoder(feature_dim, embed_dim, dropout)\n",
        "\n",
        "        # GCN layers with residual connections\n",
        "        self.gnn_layers = nn.ModuleList([\n",
        "            GCNConv(embed_dim, embed_dim) for _ in range(num_gnn_layers)\n",
        "        ])\n",
        "        self.gnn_norms = nn.ModuleList([\n",
        "            nn.LayerNorm(embed_dim) for _ in range(num_gnn_layers)\n",
        "        ])\n",
        "        self.gnn_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Prediction head: [h_student || h_step || h_kc] -> logit\n",
        "        pred_input_dim = embed_dim * 3\n",
        "        self.prediction_head = nn.Sequential(\n",
        "            nn.Linear(pred_input_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, 1)\n",
        "        )\n",
        "\n",
        "        # Cache (reset per fold)\n",
        "        self._cached_edge_index = None\n",
        "\n",
        "    def _build_edge_index(self, hetero_data, device):\n",
        "        if self._cached_edge_index is not None:\n",
        "            return self._cached_edge_index\n",
        "\n",
        "        all_edges = []\n",
        "\n",
        "        q_t = hetero_data['question', 'contains', 'step'].edge_index.clone()\n",
        "        q_t[0] += self.question_offset\n",
        "        q_t[1] += self.step_offset\n",
        "        all_edges.append(q_t)\n",
        "\n",
        "        t_q = hetero_data['step', 'belongs_to', 'question'].edge_index.clone()\n",
        "        t_q[0] += self.step_offset\n",
        "        t_q[1] += self.question_offset\n",
        "        all_edges.append(t_q)\n",
        "\n",
        "        t_c = hetero_data['step', 'requires', 'kc'].edge_index.clone()\n",
        "        t_c[0] += self.step_offset\n",
        "        t_c[1] += self.kc_offset\n",
        "        all_edges.append(t_c)\n",
        "\n",
        "        c_t = hetero_data['kc', 'required_by', 'step'].edge_index.clone()\n",
        "        c_t[0] += self.kc_offset\n",
        "        c_t[1] += self.step_offset\n",
        "        all_edges.append(c_t)\n",
        "\n",
        "        s_q = hetero_data['student', 'attempted', 'question'].edge_index.clone()\n",
        "        s_q[1] += self.question_offset\n",
        "        all_edges.append(s_q)\n",
        "\n",
        "        q_s = hetero_data['question', 'attempted_by', 'student'].edge_index.clone()\n",
        "        q_s[0] += self.question_offset\n",
        "        all_edges.append(q_s)\n",
        "\n",
        "        self._cached_edge_index = torch.cat(all_edges, dim=1).to(device)\n",
        "        return self._cached_edge_index\n",
        "\n",
        "    def forward(self, hetero_data, student_idx, step_idx, kc_idx, device):\n",
        "        # Encode node features\n",
        "        h_s = self.student_encoder(hetero_data['student'].x.to(device))\n",
        "        h_q = self.question_encoder(hetero_data['question'].x.to(device))\n",
        "        h_t = self.step_encoder(hetero_data['step'].x.to(device))\n",
        "        h_c = self.kc_encoder(hetero_data['kc'].x.to(device))\n",
        "\n",
        "        H = torch.cat([h_s, h_q, h_t, h_c], dim=0)\n",
        "        edge_index = self._build_edge_index(hetero_data, device)\n",
        "\n",
        "        # GCN with residuals\n",
        "        for gnn, norm in zip(self.gnn_layers, self.gnn_norms):\n",
        "            H_new = gnn(H, edge_index)\n",
        "            H_new = norm(H_new)\n",
        "            H_new = F.relu(H_new)\n",
        "            H_new = self.gnn_dropout(H_new)\n",
        "            H = H + H_new\n",
        "\n",
        "        # Extract batch embeddings\n",
        "        h_students = H[student_idx + self.student_offset]\n",
        "        h_steps = H[step_idx + self.step_offset]\n",
        "        h_kcs = H[kc_idx + self.kc_offset]\n",
        "\n",
        "        # Predict\n",
        "        combined = torch.cat([h_students, h_steps, h_kcs], dim=-1)\n",
        "        logits = self.prediction_head(combined).squeeze(-1)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# FUNCTION 5: Dataset\n",
        "# ============================================================\n",
        "\n",
        "class KTDatasetPure(Dataset):\n",
        "    def __init__(self, df, stu2idx, t2idx, c2idx,\n",
        "                 unk_student_idx, unk_step_idx, unk_kc_idx):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.stu2idx = stu2idx\n",
        "        self.t2idx = t2idx\n",
        "        self.c2idx = c2idx\n",
        "        self.unk_student_idx = unk_student_idx\n",
        "        self.unk_step_idx = unk_step_idx\n",
        "        self.unk_kc_idx = unk_kc_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        student_idx = self.stu2idx.get(row['student_id'], self.unk_student_idx)\n",
        "        step_idx = self.t2idx.get(row['step_id'], self.unk_step_idx)\n",
        "        kc_idx = self.c2idx.get(row['kc_id'], self.unk_kc_idx)\n",
        "        label = torch.tensor(row['correct'], dtype=torch.float32)\n",
        "        return student_idx, step_idx, kc_idx, label\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# FUNCTION 6: Training & Evaluation\n",
        "# ============================================================\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, min_delta=0.001):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.best_state = None\n",
        "\n",
        "    def __call__(self, score, model):\n",
        "        if self.best_score is None or score > self.best_score + self.min_delta:\n",
        "            self.best_score = score\n",
        "            self.best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "    def load_best(self, model):\n",
        "        if self.best_state:\n",
        "            model.load_state_dict(self.best_state)\n",
        "\n",
        "\n",
        "def train_epoch(model, loader, optimizer, criterion, hetero_data, device, grad_clip=1.0):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    for batch in loader:\n",
        "        student_idx, step_idx, kc_idx, labels = batch\n",
        "        student_idx = student_idx.to(device)\n",
        "        step_idx = step_idx.to(device)\n",
        "        kc_idx = kc_idx.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(hetero_data, student_idx, step_idx, kc_idx, device)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * len(labels)\n",
        "        all_preds.extend(torch.sigmoid(logits).detach().cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(loader.dataset)\n",
        "    auc = roc_auc_score(all_labels, all_preds)\n",
        "    acc = accuracy_score(all_labels, np.array(all_preds) > 0.5)\n",
        "    return avg_loss, auc, acc\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion, hetero_data, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    for batch in loader:\n",
        "        student_idx, step_idx, kc_idx, labels = batch\n",
        "        student_idx = student_idx.to(device)\n",
        "        step_idx = step_idx.to(device)\n",
        "        kc_idx = kc_idx.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        logits = model(hetero_data, student_idx, step_idx, kc_idx, device)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        total_loss += loss.item() * len(labels)\n",
        "        all_preds.extend(torch.sigmoid(logits).cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(loader.dataset)\n",
        "    auc = roc_auc_score(all_labels, all_preds)\n",
        "    acc = accuracy_score(all_labels, np.array(all_preds) > 0.5)\n",
        "    return avg_loss, auc, acc\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# FUNCTION 7: Full Single-Fold Pipeline\n",
        "# ============================================================\n",
        "\n",
        "def run_single_fold(df_train_fold, df_val_fold, config, fold_num=None, verbose=True):\n",
        "    \"\"\"\n",
        "    Run the complete pipeline for one fold:\n",
        "    1. Build entity mappings from df_train_fold\n",
        "    2. Build graph from df_train_fold\n",
        "    3. Compute node features from df_train_fold\n",
        "    4. Initialize fresh model\n",
        "    5. Train with early stopping on val AUC\n",
        "    6. Return results dict\n",
        "    \"\"\"\n",
        "    device = config.DEVICE\n",
        "    prefix = f\"[Fold {fold_num}] \" if fold_num is not None else \"\"\n",
        "\n",
        "    # --- Step 1: Entity mappings ---\n",
        "    mappings, entity_counts, unk_indices = build_entity_mappings(df_train_fold)\n",
        "    if verbose:\n",
        "        print(f\"{prefix}Entities: S={entity_counts['num_students']}, \"\n",
        "              f\"Q={entity_counts['num_questions']}, \"\n",
        "              f\"T={entity_counts['num_steps']}, \"\n",
        "              f\"C={entity_counts['num_kcs']}\")\n",
        "\n",
        "    # --- Step 2: Build graph ---\n",
        "    hetero_data, total_edges = build_graph(df_train_fold, mappings, entity_counts, unk_indices)\n",
        "    if verbose:\n",
        "        print(f\"{prefix}Graph edges: {total_edges:,}\")\n",
        "\n",
        "    # --- Step 3: Node features ---\n",
        "    feat_tensors = compute_node_features(df_train_fold, mappings, entity_counts, unk_indices)\n",
        "    hetero_data['student'].x = feat_tensors['student']\n",
        "    hetero_data['question'].x = feat_tensors['question']\n",
        "    hetero_data['step'].x = feat_tensors['step']\n",
        "    hetero_data['kc'].x = feat_tensors['kc']\n",
        "\n",
        "    # --- Step 4: DataLoaders ---\n",
        "    train_dataset = KTDatasetPure(\n",
        "        df_train_fold, mappings['stu2idx'], mappings['t2idx'], mappings['c2idx'],\n",
        "        unk_indices['student'], unk_indices['step'], unk_indices['kc']\n",
        "    )\n",
        "    val_dataset = KTDatasetPure(\n",
        "        df_val_fold, mappings['stu2idx'], mappings['t2idx'], mappings['c2idx'],\n",
        "        unk_indices['student'], unk_indices['step'], unk_indices['kc']\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True,\n",
        "                              pin_memory=True, num_workers=0)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False,\n",
        "                            pin_memory=True, num_workers=0)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"{prefix}Train: {len(train_loader.dataset):,} samples, \"\n",
        "              f\"Val: {len(val_loader.dataset):,} samples\")\n",
        "\n",
        "    # --- Step 5: Model ---\n",
        "    model = GCNPure(\n",
        "        num_students=entity_counts['num_students'],\n",
        "        num_questions=entity_counts['num_questions'],\n",
        "        num_steps=entity_counts['num_steps'],\n",
        "        num_kcs=entity_counts['num_kcs'],\n",
        "        feature_dim=NUM_FEATURES,\n",
        "        embed_dim=config.EMBED_DIM,\n",
        "        hidden_dim=config.HIDDEN_DIM,\n",
        "        num_gnn_layers=config.NUM_GNN_LAYERS,\n",
        "        dropout=config.DROPOUT\n",
        "    ).to(device)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    if verbose:\n",
        "        print(f\"{prefix}Model params: {total_params:,}\")\n",
        "\n",
        "    # --- Step 6: Training setup ---\n",
        "    # Compute pos_weight from THIS fold's training data\n",
        "    n_correct = df_train_fold['correct'].sum()\n",
        "    n_incorrect = len(df_train_fold) - n_correct\n",
        "    pos_weight = torch.tensor([n_incorrect / n_correct], dtype=torch.float32).to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE,\n",
        "                                   weight_decay=config.WEIGHT_DECAY)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='max', factor=0.5, patience=5\n",
        "    )\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "    early_stopping = EarlyStopping(patience=config.PATIENCE)\n",
        "\n",
        "    # --- Step 7: Training loop ---\n",
        "    history = defaultdict(list)\n",
        "    best_val_auc = 0\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"{prefix}Training (pos_weight={pos_weight.item():.4f})...\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "    for epoch in range(config.EPOCHS):\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        train_loss, train_auc, train_acc = train_epoch(\n",
        "            model, train_loader, optimizer, criterion, hetero_data, device, config.GRAD_CLIP\n",
        "        )\n",
        "        val_loss, val_auc, val_acc = evaluate(\n",
        "            model, val_loader, criterion, hetero_data, device\n",
        "        )\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_auc'].append(train_auc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_auc'].append(val_auc)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        marker = \" ★\" if val_auc > best_val_auc else \"\"\n",
        "        if val_auc > best_val_auc:\n",
        "            best_val_auc = val_auc\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"{prefix}Epoch {epoch+1:3d}/{config.EPOCHS} | \"\n",
        "                  f\"Train AUC: {train_auc:.4f} | Val AUC: {val_auc:.4f} | \"\n",
        "                  f\"Val Acc: {val_acc:.4f} | Time: {epoch_time:.1f}s{marker}\")\n",
        "\n",
        "        scheduler.step(val_auc)\n",
        "        early_stopping(val_auc, model)\n",
        "\n",
        "        if early_stopping.early_stop:\n",
        "            if verbose:\n",
        "                print(f\"{prefix}Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    # Load best model\n",
        "    early_stopping.load_best(model)\n",
        "\n",
        "    # Final evaluation on val with best model\n",
        "    final_val_loss, final_val_auc, final_val_acc = evaluate(\n",
        "        model, val_loader, criterion, hetero_data, device\n",
        "    )\n",
        "\n",
        "    stopped_epoch = len(history['train_loss'])\n",
        "\n",
        "    if verbose:\n",
        "        print(\"-\" * 70)\n",
        "        print(f\"{prefix}Best Val AUC: {final_val_auc:.4f} | \"\n",
        "              f\"Val Acc: {final_val_acc:.4f} | Stopped at epoch: {stopped_epoch}\")\n",
        "\n",
        "    return {\n",
        "        'val_auc': final_val_auc,\n",
        "        'val_acc': final_val_acc,\n",
        "        'val_loss': final_val_loss,\n",
        "        'train_auc': history['train_auc'][-1],\n",
        "        'stopped_epoch': stopped_epoch,\n",
        "        'total_params': total_params,\n",
        "        'history': dict(history),\n",
        "        'model_state': early_stopping.best_state,\n",
        "        'hetero_data': hetero_data,\n",
        "        'mappings': mappings,\n",
        "        'entity_counts': entity_counts,\n",
        "        'unk_indices': unk_indices,\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"✓ All pipeline functions defined\")\n",
        "print(\"  - build_entity_mappings()\")\n",
        "print(\"  - build_graph()\")\n",
        "print(\"  - compute_node_features()\")\n",
        "print(\"  - GCNPure model\")\n",
        "print(\"  - KTDatasetPure dataset\")\n",
        "print(\"  - train_epoch() / evaluate()\")\n",
        "print(\"  - run_single_fold()\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cWsbyFg-o-po",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cb87954-7f72-4db5-d167-f5592b2fe951"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "5-FOLD STUDENT-LEVEL CROSS-VALIDATION\n",
            "============================================================\n",
            "\n",
            "Protocol:\n",
            "  - 453 non-test students split into 5 folds\n",
            "  - Each fold: full pipeline rebuild (mappings, graph, features, model)\n",
            "  - All design decisions FROZEN before CV\n",
            "  - TEST set (81 students) completely untouched\n",
            "\n",
            "Frozen hyperparameters:\n",
            "  EMBED_DIM=32, HIDDEN_DIM=64\n",
            "  NUM_GNN_LAYERS=2, DROPOUT=0.2\n",
            "  LR=0.001, WEIGHT_DECAY=0.01\n",
            "  BATCH_SIZE=512, PATIENCE=10\n",
            "\n",
            "\n",
            "============================================================\n",
            "FOLD 1 / 5\n",
            "============================================================\n",
            "Train students: 362, Val students: 91\n",
            "Train interactions: 505,387, Val interactions: 173,698\n",
            "[Fold 1] Entities: S=363, Q=1074, T=142722, C=399\n",
            "[Fold 1] Graph edges: 646,134\n",
            "[Fold 1] Train: 505,387 samples, Val: 173,698 samples\n",
            "[Fold 1] Model params: 16,193\n",
            "[Fold 1] Training (pos_weight=0.3005)...\n",
            "----------------------------------------------------------------------\n",
            "[Fold 1] Epoch   1/100 | Train AUC: 0.9033 | Val AUC: 0.7644 | Val Acc: 0.7689 | Time: 90.4s ★\n",
            "[Fold 1] Epoch   2/100 | Train AUC: 0.9139 | Val AUC: 0.7644 | Val Acc: 0.7345 | Time: 88.8s\n",
            "[Fold 1] Epoch   3/100 | Train AUC: 0.9151 | Val AUC: 0.7683 | Val Acc: 0.7501 | Time: 88.5s ★\n",
            "[Fold 1] Epoch   4/100 | Train AUC: 0.9156 | Val AUC: 0.7660 | Val Acc: 0.7378 | Time: 88.5s\n",
            "[Fold 1] Epoch   5/100 | Train AUC: 0.9161 | Val AUC: 0.7649 | Val Acc: 0.7537 | Time: 88.5s\n",
            "[Fold 1] Epoch   6/100 | Train AUC: 0.9166 | Val AUC: 0.7614 | Val Acc: 0.7552 | Time: 88.5s\n",
            "[Fold 1] Epoch   7/100 | Train AUC: 0.9169 | Val AUC: 0.7547 | Val Acc: 0.7417 | Time: 88.4s\n",
            "[Fold 1] Epoch   8/100 | Train AUC: 0.9173 | Val AUC: 0.7401 | Val Acc: 0.7425 | Time: 88.5s\n",
            "[Fold 1] Epoch   9/100 | Train AUC: 0.9175 | Val AUC: 0.7318 | Val Acc: 0.7569 | Time: 88.3s\n",
            "[Fold 1] Epoch  10/100 | Train AUC: 0.9181 | Val AUC: 0.7349 | Val Acc: 0.7485 | Time: 88.6s\n",
            "[Fold 1] Epoch  11/100 | Train AUC: 0.9182 | Val AUC: 0.7312 | Val Acc: 0.7422 | Time: 88.7s\n",
            "[Fold 1] Epoch  12/100 | Train AUC: 0.9183 | Val AUC: 0.7280 | Val Acc: 0.7247 | Time: 88.8s\n",
            "[Fold 1] Epoch  13/100 | Train AUC: 0.9184 | Val AUC: 0.7321 | Val Acc: 0.7352 | Time: 88.8s\n",
            "[Fold 1] Early stopping at epoch 13\n",
            "----------------------------------------------------------------------\n",
            "[Fold 1] Best Val AUC: 0.7682 | Val Acc: 0.7501 | Stopped at epoch: 13\n",
            "\n",
            "Fold 1 completed in 1242.6s\n",
            "\n",
            "============================================================\n",
            "FOLD 2 / 5\n",
            "============================================================\n",
            "Train students: 362, Val students: 91\n",
            "Train interactions: 556,468, Val interactions: 122,617\n",
            "[Fold 2] Entities: S=363, Q=1073, T=155183, C=409\n",
            "[Fold 2] Graph edges: 702,406\n",
            "[Fold 2] Train: 556,468 samples, Val: 122,617 samples\n",
            "[Fold 2] Model params: 16,193\n",
            "[Fold 2] Training (pos_weight=0.3018)...\n",
            "----------------------------------------------------------------------\n",
            "[Fold 2] Epoch   1/100 | Train AUC: 0.9058 | Val AUC: 0.7468 | Val Acc: 0.6942 | Time: 91.0s ★\n",
            "[Fold 2] Epoch   2/100 | Train AUC: 0.9146 | Val AUC: 0.7706 | Val Acc: 0.5690 | Time: 90.9s ★\n",
            "[Fold 2] Epoch   3/100 | Train AUC: 0.9155 | Val AUC: 0.7697 | Val Acc: 0.6738 | Time: 90.8s\n",
            "[Fold 2] Epoch   4/100 | Train AUC: 0.9161 | Val AUC: 0.7708 | Val Acc: 0.6531 | Time: 91.5s ★\n",
            "[Fold 2] Epoch   5/100 | Train AUC: 0.9166 | Val AUC: 0.7720 | Val Acc: 0.6958 | Time: 90.9s ★\n",
            "[Fold 2] Epoch   6/100 | Train AUC: 0.9167 | Val AUC: 0.7745 | Val Acc: 0.7451 | Time: 90.7s ★\n",
            "[Fold 2] Epoch   7/100 | Train AUC: 0.9169 | Val AUC: 0.7632 | Val Acc: 0.5607 | Time: 91.2s\n",
            "[Fold 2] Epoch   8/100 | Train AUC: 0.9174 | Val AUC: 0.7707 | Val Acc: 0.7013 | Time: 90.8s\n",
            "[Fold 2] Epoch   9/100 | Train AUC: 0.9174 | Val AUC: 0.7620 | Val Acc: 0.7202 | Time: 90.8s\n",
            "[Fold 2] Epoch  10/100 | Train AUC: 0.9177 | Val AUC: 0.7627 | Val Acc: 0.7092 | Time: 90.4s\n",
            "[Fold 2] Epoch  11/100 | Train AUC: 0.9178 | Val AUC: 0.7701 | Val Acc: 0.7157 | Time: 90.6s\n",
            "[Fold 2] Epoch  12/100 | Train AUC: 0.9180 | Val AUC: 0.7669 | Val Acc: 0.7251 | Time: 90.8s\n",
            "[Fold 2] Epoch  13/100 | Train AUC: 0.9187 | Val AUC: 0.7582 | Val Acc: 0.7183 | Time: 90.5s\n",
            "[Fold 2] Epoch  14/100 | Train AUC: 0.9189 | Val AUC: 0.7680 | Val Acc: 0.6963 | Time: 90.1s\n",
            "[Fold 2] Epoch  15/100 | Train AUC: 0.9189 | Val AUC: 0.7557 | Val Acc: 0.7068 | Time: 90.8s\n",
            "[Fold 2] Epoch  16/100 | Train AUC: 0.9189 | Val AUC: 0.7499 | Val Acc: 0.7194 | Time: 90.3s\n",
            "[Fold 2] Early stopping at epoch 16\n",
            "----------------------------------------------------------------------\n",
            "[Fold 2] Best Val AUC: 0.7744 | Val Acc: 0.7451 | Stopped at epoch: 16\n",
            "\n",
            "Fold 2 completed in 1541.3s\n",
            "\n",
            "============================================================\n",
            "FOLD 3 / 5\n",
            "============================================================\n",
            "Train students: 362, Val students: 91\n",
            "Train interactions: 546,376, Val interactions: 132,709\n",
            "[Fold 3] Entities: S=363, Q=1074, T=151572, C=402\n",
            "[Fold 3] Graph edges: 686,182\n",
            "[Fold 3] Train: 546,376 samples, Val: 132,709 samples\n",
            "[Fold 3] Model params: 16,193\n",
            "[Fold 3] Training (pos_weight=0.3052)...\n",
            "----------------------------------------------------------------------\n",
            "[Fold 3] Epoch   1/100 | Train AUC: 0.9055 | Val AUC: 0.7621 | Val Acc: 0.7543 | Time: 90.0s ★\n",
            "[Fold 3] Epoch   2/100 | Train AUC: 0.9147 | Val AUC: 0.7643 | Val Acc: 0.7538 | Time: 90.3s ★\n",
            "[Fold 3] Epoch   3/100 | Train AUC: 0.9159 | Val AUC: 0.7613 | Val Acc: 0.7492 | Time: 90.3s\n",
            "[Fold 3] Epoch   4/100 | Train AUC: 0.9166 | Val AUC: 0.7579 | Val Acc: 0.7346 | Time: 89.8s\n",
            "[Fold 3] Epoch   5/100 | Train AUC: 0.9170 | Val AUC: 0.7547 | Val Acc: 0.7559 | Time: 90.3s\n",
            "[Fold 3] Epoch   6/100 | Train AUC: 0.9173 | Val AUC: 0.7588 | Val Acc: 0.7610 | Time: 90.4s\n",
            "[Fold 3] Epoch   7/100 | Train AUC: 0.9176 | Val AUC: 0.7530 | Val Acc: 0.7486 | Time: 89.9s\n",
            "[Fold 3] Epoch   8/100 | Train AUC: 0.9180 | Val AUC: 0.7364 | Val Acc: 0.7384 | Time: 90.0s\n",
            "[Fold 3] Epoch   9/100 | Train AUC: 0.9187 | Val AUC: 0.7365 | Val Acc: 0.7735 | Time: 90.1s\n",
            "[Fold 3] Epoch  10/100 | Train AUC: 0.9189 | Val AUC: 0.7363 | Val Acc: 0.7487 | Time: 90.1s\n",
            "[Fold 3] Epoch  11/100 | Train AUC: 0.9190 | Val AUC: 0.7337 | Val Acc: 0.7582 | Time: 90.0s\n",
            "[Fold 3] Epoch  12/100 | Train AUC: 0.9191 | Val AUC: 0.7389 | Val Acc: 0.7511 | Time: 90.1s\n",
            "[Fold 3] Early stopping at epoch 12\n",
            "----------------------------------------------------------------------\n",
            "[Fold 3] Best Val AUC: 0.7643 | Val Acc: 0.7538 | Stopped at epoch: 12\n",
            "\n",
            "Fold 3 completed in 1169.2s\n",
            "\n",
            "============================================================\n",
            "FOLD 4 / 5\n",
            "============================================================\n",
            "Train students: 363, Val students: 90\n",
            "Train interactions: 562,082, Val interactions: 117,003\n",
            "[Fold 4] Entities: S=364, Q=1076, T=153229, C=417\n",
            "[Fold 4] Graph edges: 695,766\n",
            "[Fold 4] Train: 562,082 samples, Val: 117,003 samples\n",
            "[Fold 4] Model params: 16,193\n",
            "[Fold 4] Training (pos_weight=0.3004)...\n",
            "----------------------------------------------------------------------\n",
            "[Fold 4] Epoch   1/100 | Train AUC: 0.9029 | Val AUC: 0.7295 | Val Acc: 0.7145 | Time: 91.1s ★\n",
            "[Fold 4] Epoch   2/100 | Train AUC: 0.9131 | Val AUC: 0.7207 | Val Acc: 0.7263 | Time: 90.2s\n",
            "[Fold 4] Epoch   3/100 | Train AUC: 0.9145 | Val AUC: 0.7315 | Val Acc: 0.7140 | Time: 90.8s ★\n",
            "[Fold 4] Epoch   4/100 | Train AUC: 0.9153 | Val AUC: 0.7358 | Val Acc: 0.7248 | Time: 91.1s ★\n",
            "[Fold 4] Epoch   5/100 | Train AUC: 0.9155 | Val AUC: 0.7204 | Val Acc: 0.7353 | Time: 90.4s\n",
            "[Fold 4] Epoch   6/100 | Train AUC: 0.9161 | Val AUC: 0.7165 | Val Acc: 0.7415 | Time: 91.0s\n",
            "[Fold 4] Epoch   7/100 | Train AUC: 0.9162 | Val AUC: 0.7155 | Val Acc: 0.7355 | Time: 90.9s\n",
            "[Fold 4] Epoch   8/100 | Train AUC: 0.9165 | Val AUC: 0.7229 | Val Acc: 0.7422 | Time: 91.8s\n",
            "[Fold 4] Epoch   9/100 | Train AUC: 0.9166 | Val AUC: 0.7268 | Val Acc: 0.7300 | Time: 90.9s\n",
            "[Fold 4] Epoch  10/100 | Train AUC: 0.9169 | Val AUC: 0.7234 | Val Acc: 0.7368 | Time: 90.7s\n",
            "[Fold 4] Epoch  11/100 | Train AUC: 0.9176 | Val AUC: 0.7193 | Val Acc: 0.7377 | Time: 91.5s\n",
            "[Fold 4] Epoch  12/100 | Train AUC: 0.9176 | Val AUC: 0.7187 | Val Acc: 0.7387 | Time: 91.1s\n",
            "[Fold 4] Epoch  13/100 | Train AUC: 0.9179 | Val AUC: 0.7164 | Val Acc: 0.7623 | Time: 90.8s\n",
            "[Fold 4] Epoch  14/100 | Train AUC: 0.9178 | Val AUC: 0.7205 | Val Acc: 0.7446 | Time: 91.0s\n",
            "[Fold 4] Early stopping at epoch 14\n",
            "----------------------------------------------------------------------\n",
            "[Fold 4] Best Val AUC: 0.7359 | Val Acc: 0.7248 | Stopped at epoch: 14\n",
            "\n",
            "Fold 4 completed in 1360.5s\n",
            "\n",
            "============================================================\n",
            "FOLD 5 / 5\n",
            "============================================================\n",
            "Train students: 363, Val students: 90\n",
            "Train interactions: 546,027, Val interactions: 133,058\n",
            "[Fold 5] Entities: S=364, Q=1069, T=151788, C=408\n",
            "[Fold 5] Graph edges: 687,282\n",
            "[Fold 5] Train: 546,027 samples, Val: 133,058 samples\n",
            "[Fold 5] Model params: 16,193\n",
            "[Fold 5] Training (pos_weight=0.2995)...\n",
            "----------------------------------------------------------------------\n",
            "[Fold 5] Epoch   1/100 | Train AUC: 0.9031 | Val AUC: 0.7616 | Val Acc: 0.7458 | Time: 90.7s ★\n",
            "[Fold 5] Epoch   2/100 | Train AUC: 0.9135 | Val AUC: 0.7623 | Val Acc: 0.7567 | Time: 90.6s ★\n",
            "[Fold 5] Epoch   3/100 | Train AUC: 0.9145 | Val AUC: 0.7643 | Val Acc: 0.7613 | Time: 90.8s ★\n",
            "[Fold 5] Epoch   4/100 | Train AUC: 0.9151 | Val AUC: 0.7627 | Val Acc: 0.7503 | Time: 90.4s\n",
            "[Fold 5] Epoch   5/100 | Train AUC: 0.9155 | Val AUC: 0.7642 | Val Acc: 0.7662 | Time: 90.9s\n",
            "[Fold 5] Epoch   6/100 | Train AUC: 0.9159 | Val AUC: 0.7654 | Val Acc: 0.7562 | Time: 90.3s ★\n",
            "[Fold 5] Epoch   7/100 | Train AUC: 0.9161 | Val AUC: 0.7623 | Val Acc: 0.7621 | Time: 90.8s\n",
            "[Fold 5] Epoch   8/100 | Train AUC: 0.9164 | Val AUC: 0.7516 | Val Acc: 0.7608 | Time: 90.5s\n",
            "[Fold 5] Epoch   9/100 | Train AUC: 0.9166 | Val AUC: 0.7453 | Val Acc: 0.7790 | Time: 90.4s\n",
            "[Fold 5] Epoch  10/100 | Train AUC: 0.9166 | Val AUC: 0.7578 | Val Acc: 0.7626 | Time: 90.6s\n",
            "[Fold 5] Epoch  11/100 | Train AUC: 0.9170 | Val AUC: 0.7529 | Val Acc: 0.7481 | Time: 90.6s\n",
            "[Fold 5] Epoch  12/100 | Train AUC: 0.9171 | Val AUC: 0.7344 | Val Acc: 0.7514 | Time: 90.1s\n",
            "[Fold 5] Epoch  13/100 | Train AUC: 0.9176 | Val AUC: 0.7376 | Val Acc: 0.7479 | Time: 90.4s\n",
            "[Fold 5] Epoch  14/100 | Train AUC: 0.9178 | Val AUC: 0.7267 | Val Acc: 0.7591 | Time: 90.5s\n",
            "[Fold 5] Epoch  15/100 | Train AUC: 0.9179 | Val AUC: 0.7296 | Val Acc: 0.7464 | Time: 90.5s\n",
            "[Fold 5] Epoch  16/100 | Train AUC: 0.9178 | Val AUC: 0.7261 | Val Acc: 0.7535 | Time: 90.5s\n",
            "[Fold 5] Early stopping at epoch 16\n",
            "----------------------------------------------------------------------\n",
            "[Fold 5] Best Val AUC: 0.7654 | Val Acc: 0.7562 | Stopped at epoch: 16\n",
            "\n",
            "Fold 5 completed in 1537.0s\n",
            "\n",
            "============================================================\n",
            "5-FOLD CROSS-VALIDATION RESULTS\n",
            "============================================================\n",
            "\n",
            "Fold     Val AUC      Val Acc      Epochs    \n",
            "------------------------------------------\n",
            "Fold 1   0.7682       0.7501       13        \n",
            "Fold 2   0.7744       0.7451       16        \n",
            "Fold 3   0.7643       0.7538       12        \n",
            "Fold 4   0.7359       0.7248       14        \n",
            "Fold 5   0.7654       0.7562       16        \n",
            "------------------------------------------\n",
            "Mean     0.7617       0.7460       14.2      \n",
            "Std      0.0133       0.0112       1.6       \n",
            "Min      0.7359       0.7248       12        \n",
            "Max      0.7744       0.7562       16        \n",
            "\n",
            "╔══════════════════════════════════════════════╗\n",
            "║  CV Val AUC: 0.7617 ± 0.0133              ║\n",
            "║  CV Val Acc: 0.7460 ± 0.0112              ║\n",
            "╚══════════════════════════════════════════════╝\n",
            "\n",
            "Total CV time: 6850.6s (114.2 min)\n",
            "\n",
            "Average stopping epoch: 14 (will use for final training)\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Cell 6: Run 5-Fold Student-Level Cross-Validation\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"5-FOLD STUDENT-LEVEL CROSS-VALIDATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\"\"\n",
        "Protocol:\n",
        "  - {len(non_test_students)} non-test students split into 5 folds\n",
        "  - Each fold: full pipeline rebuild (mappings, graph, features, model)\n",
        "  - All design decisions FROZEN before CV\n",
        "  - TEST set ({len(test_students)} students) completely untouched\n",
        "\n",
        "Frozen hyperparameters:\n",
        "  EMBED_DIM={config.EMBED_DIM}, HIDDEN_DIM={config.HIDDEN_DIM}\n",
        "  NUM_GNN_LAYERS={config.NUM_GNN_LAYERS}, DROPOUT={config.DROPOUT}\n",
        "  LR={config.LEARNING_RATE}, WEIGHT_DECAY={config.WEIGHT_DECAY}\n",
        "  BATCH_SIZE=512, PATIENCE={config.PATIENCE}\n",
        "\"\"\")\n",
        "\n",
        "cv_results = []\n",
        "cv_start = time.time()\n",
        "\n",
        "for fold_idx in range(5):\n",
        "    fold_start = time.time()\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"FOLD {fold_idx + 1} / 5\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Get this fold's students\n",
        "    train_students_fold = fold_assignments[fold_idx]['train_students']\n",
        "    val_students_fold = fold_assignments[fold_idx]['val_students']\n",
        "\n",
        "    # Create dataframes for this fold\n",
        "    df_train_fold = df_non_test[df_non_test['student_id'].isin(train_students_fold)].copy()\n",
        "    df_val_fold = df_non_test[df_non_test['student_id'].isin(val_students_fold)].copy()\n",
        "\n",
        "    print(f\"Train students: {len(train_students_fold)}, \"\n",
        "          f\"Val students: {len(val_students_fold)}\")\n",
        "    print(f\"Train interactions: {len(df_train_fold):,}, \"\n",
        "          f\"Val interactions: {len(df_val_fold):,}\")\n",
        "\n",
        "    # Run full pipeline for this fold\n",
        "    result = run_single_fold(\n",
        "        df_train_fold, df_val_fold, config,\n",
        "        fold_num=fold_idx + 1, verbose=True\n",
        "    )\n",
        "\n",
        "    fold_time = time.time() - fold_start\n",
        "    result['fold_time'] = fold_time\n",
        "    cv_results.append(result)\n",
        "\n",
        "    print(f\"\\nFold {fold_idx + 1} completed in {fold_time:.1f}s\")\n",
        "\n",
        "total_cv_time = time.time() - cv_start\n",
        "\n",
        "# ============================================================\n",
        "# CV Summary\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"5-FOLD CROSS-VALIDATION RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "val_aucs = [r['val_auc'] for r in cv_results]\n",
        "val_accs = [r['val_acc'] for r in cv_results]\n",
        "stopped_epochs = [r['stopped_epoch'] for r in cv_results]\n",
        "\n",
        "print(f\"\\n{'Fold':<8} {'Val AUC':<12} {'Val Acc':<12} {'Epochs':<10}\")\n",
        "print(\"-\" * 42)\n",
        "for i, r in enumerate(cv_results):\n",
        "    print(f\"Fold {i+1:<3} {r['val_auc']:<12.4f} {r['val_acc']:<12.4f} {r['stopped_epoch']:<10}\")\n",
        "\n",
        "print(\"-\" * 42)\n",
        "print(f\"{'Mean':<8} {np.mean(val_aucs):<12.4f} {np.mean(val_accs):<12.4f} {np.mean(stopped_epochs):<10.1f}\")\n",
        "print(f\"{'Std':<8} {np.std(val_aucs):<12.4f} {np.std(val_accs):<12.4f} {np.std(stopped_epochs):<10.1f}\")\n",
        "print(f\"{'Min':<8} {np.min(val_aucs):<12.4f} {np.min(val_accs):<12.4f} {np.min(stopped_epochs):<10}\")\n",
        "print(f\"{'Max':<8} {np.max(val_aucs):<12.4f} {np.max(val_accs):<12.4f} {np.max(stopped_epochs):<10}\")\n",
        "\n",
        "print(f\"\\n╔══════════════════════════════════════════════╗\")\n",
        "print(f\"║  CV Val AUC: {np.mean(val_aucs):.4f} ± {np.std(val_aucs):.4f}              ║\")\n",
        "print(f\"║  CV Val Acc: {np.mean(val_accs):.4f} ± {np.std(val_accs):.4f}              ║\")\n",
        "print(f\"╚══════════════════════════════════════════════╝\")\n",
        "\n",
        "print(f\"\\nTotal CV time: {total_cv_time:.1f}s ({total_cv_time/60:.1f} min)\")\n",
        "\n",
        "# Store average epochs for final training\n",
        "avg_epochs_cv = int(np.mean(stopped_epochs))\n",
        "print(f\"\\nAverage stopping epoch: {avg_epochs_cv} (will use for final training)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RDUnF4Kto-pp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d013fc2c-3571-42f7-a81a-50217411f561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "FINAL TEST EVALUATION\n",
            "============================================================\n",
            "\n",
            "Protocol:\n",
            "  - Train on ALL 453 non-test students (no validation split)\n",
            "  - Train for 14 epochs (average from CV, no early stopping)\n",
            "  - Evaluate on held-out TEST set (81 students)\n",
            "  - This number appears in the paper as TEST performance\n",
            "\n",
            "[1/5] Building entity mappings from all non-test data...\n",
            "  S=454, Q=1080, T=181902, C=425\n",
            "[2/5] Building graph...\n",
            "  Edges: 827,484\n",
            "[3/5] Computing node features...\n",
            "[4/5] Creating dataloaders...\n",
            "  Train: 679,085 samples\n",
            "  Test:  111,057 samples\n",
            "[5/5] Training final model...\n",
            "\n",
            "Training for 14 epochs (CV average)...\n",
            "----------------------------------------------------------------------\n",
            "Epoch   1/14 | Train Loss: 0.1798 | Train AUC: 0.9016 | Train Acc: 0.8042 | Time: 96.5s\n",
            "Epoch   5/14 | Train Loss: 0.1670 | Train AUC: 0.9137 | Train Acc: 0.8147 | Time: 95.2s\n",
            "Epoch  10/14 | Train Loss: 0.1656 | Train AUC: 0.9150 | Train Acc: 0.8177 | Time: 96.6s\n",
            "Epoch  14/14 | Train Loss: 0.1650 | Train AUC: 0.9156 | Train Acc: 0.8179 | Time: 96.6s\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Evaluating on TEST set...\n",
            "\n",
            "============================================================\n",
            "FINAL RESULTS\n",
            "============================================================\n",
            "\n",
            "╔══════════════════════════════════════════════════════════════╗\n",
            "║                    GCNPure - GraphKT Baseline                ║\n",
            "║                    Algebra 2005-2006 Dataset                 ║\n",
            "╠══════════════════════════════════════════════════════════════╣\n",
            "║                                                              ║\n",
            "║  5-Fold CV Validation:                                       ║\n",
            "║    AUC:      0.7617 ± 0.0133                              ║\n",
            "║    Accuracy: 0.7460 ± 0.0112                              ║\n",
            "║                                                              ║\n",
            "║  Test Set (held-out, 81 students):                          ║\n",
            "║    AUC:      0.7255                                        ║\n",
            "║    Accuracy: 0.7297                                        ║\n",
            "║                                                              ║\n",
            "║  Model: 16,193 parameters                              ║\n",
            "║  Training: 14 epochs (CV average)                           ║\n",
            "║  Split: Student-level (Split B), no leakage                  ║\n",
            "║                                                              ║\n",
            "╚══════════════════════════════════════════════════════════════╝\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Cell 7: Final TEST Set Evaluation\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FINAL TEST EVALUATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\"\"\n",
        "Protocol:\n",
        "  - Train on ALL {len(non_test_students)} non-test students (no validation split)\n",
        "  - Train for {avg_epochs_cv} epochs (average from CV, no early stopping)\n",
        "  - Evaluate on held-out TEST set ({len(test_students)} students)\n",
        "  - This number appears in the paper as TEST performance\n",
        "\"\"\")\n",
        "\n",
        "# --- Train on all non-test data ---\n",
        "df_train_final = df_non_test.copy()\n",
        "\n",
        "print(\"[1/5] Building entity mappings from all non-test data...\")\n",
        "mappings_final, entity_counts_final, unk_indices_final = build_entity_mappings(df_train_final)\n",
        "print(f\"  S={entity_counts_final['num_students']}, \"\n",
        "      f\"Q={entity_counts_final['num_questions']}, \"\n",
        "      f\"T={entity_counts_final['num_steps']}, \"\n",
        "      f\"C={entity_counts_final['num_kcs']}\")\n",
        "\n",
        "print(\"[2/5] Building graph...\")\n",
        "hetero_data_final, total_edges_final = build_graph(\n",
        "    df_train_final, mappings_final, entity_counts_final, unk_indices_final\n",
        ")\n",
        "print(f\"  Edges: {total_edges_final:,}\")\n",
        "\n",
        "print(\"[3/5] Computing node features...\")\n",
        "feat_tensors_final = compute_node_features(\n",
        "    df_train_final, mappings_final, entity_counts_final, unk_indices_final\n",
        ")\n",
        "hetero_data_final['student'].x = feat_tensors_final['student']\n",
        "hetero_data_final['question'].x = feat_tensors_final['question']\n",
        "hetero_data_final['step'].x = feat_tensors_final['step']\n",
        "hetero_data_final['kc'].x = feat_tensors_final['kc']\n",
        "\n",
        "print(\"[4/5] Creating dataloaders...\")\n",
        "train_dataset_final = KTDatasetPure(\n",
        "    df_train_final,\n",
        "    mappings_final['stu2idx'], mappings_final['t2idx'], mappings_final['c2idx'],\n",
        "    unk_indices_final['student'], unk_indices_final['step'], unk_indices_final['kc']\n",
        ")\n",
        "test_dataset_final = KTDatasetPure(\n",
        "    df_test_final,\n",
        "    mappings_final['stu2idx'], mappings_final['t2idx'], mappings_final['c2idx'],\n",
        "    unk_indices_final['student'], unk_indices_final['step'], unk_indices_final['kc']\n",
        ")\n",
        "\n",
        "train_loader_final = DataLoader(train_dataset_final, batch_size=512, shuffle=True,\n",
        "                                 pin_memory=True, num_workers=0)\n",
        "test_loader_final = DataLoader(test_dataset_final, batch_size=512, shuffle=False,\n",
        "                                pin_memory=True, num_workers=0)\n",
        "\n",
        "print(f\"  Train: {len(train_dataset_final):,} samples\")\n",
        "print(f\"  Test:  {len(test_dataset_final):,} samples\")\n",
        "\n",
        "print(\"[5/5] Training final model...\")\n",
        "device = config.DEVICE\n",
        "\n",
        "model_final = GCNPure(\n",
        "    num_students=entity_counts_final['num_students'],\n",
        "    num_questions=entity_counts_final['num_questions'],\n",
        "    num_steps=entity_counts_final['num_steps'],\n",
        "    num_kcs=entity_counts_final['num_kcs'],\n",
        "    feature_dim=NUM_FEATURES,\n",
        "    embed_dim=config.EMBED_DIM,\n",
        "    hidden_dim=config.HIDDEN_DIM,\n",
        "    num_gnn_layers=config.NUM_GNN_LAYERS,\n",
        "    dropout=config.DROPOUT\n",
        ").to(device)\n",
        "\n",
        "# Class weights from full training set\n",
        "n_correct = df_train_final['correct'].sum()\n",
        "n_incorrect = len(df_train_final) - n_correct\n",
        "pos_weight_final = torch.tensor([n_incorrect / n_correct], dtype=torch.float32).to(device)\n",
        "\n",
        "optimizer_final = torch.optim.AdamW(model_final.parameters(), lr=config.LEARNING_RATE,\n",
        "                                      weight_decay=config.WEIGHT_DECAY)\n",
        "scheduler_final = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer_final, mode='min', factor=0.5, patience=5\n",
        ")\n",
        "criterion_final = nn.BCEWithLogitsLoss(pos_weight=pos_weight_final)\n",
        "\n",
        "# Train for fixed number of epochs (from CV average)\n",
        "print(f\"\\nTraining for {avg_epochs_cv} epochs (CV average)...\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for epoch in range(avg_epochs_cv):\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    train_loss, train_auc, train_acc = train_epoch(\n",
        "        model_final, train_loader_final, optimizer_final, criterion_final,\n",
        "        hetero_data_final, device, config.GRAD_CLIP\n",
        "    )\n",
        "\n",
        "    epoch_time = time.time() - epoch_start\n",
        "\n",
        "    if (epoch + 1) % 5 == 0 or epoch == 0 or (epoch + 1) == avg_epochs_cv:\n",
        "        print(f\"Epoch {epoch+1:3d}/{avg_epochs_cv} | \"\n",
        "              f\"Train Loss: {train_loss:.4f} | Train AUC: {train_auc:.4f} | \"\n",
        "              f\"Train Acc: {train_acc:.4f} | Time: {epoch_time:.1f}s\")\n",
        "\n",
        "    scheduler_final.step(train_loss)\n",
        "\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# --- Final TEST evaluation ---\n",
        "print(\"\\nEvaluating on TEST set...\")\n",
        "test_loss, test_auc, test_acc = evaluate(\n",
        "    model_final, test_loader_final, criterion_final,\n",
        "    hetero_data_final, device\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"FINAL RESULTS\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"\"\"\n",
        "╔══════════════════════════════════════════════════════════════╗\n",
        "║                    GCNPure - GraphKT Baseline                ║\n",
        "║                    Algebra 2005-2006 Dataset                 ║\n",
        "╠══════════════════════════════════════════════════════════════╣\n",
        "║                                                              ║\n",
        "║  5-Fold CV Validation:                                       ║\n",
        "║    AUC:      {np.mean(val_aucs):.4f} ± {np.std(val_aucs):.4f}                              ║\n",
        "║    Accuracy: {np.mean(val_accs):.4f} ± {np.std(val_accs):.4f}                              ║\n",
        "║                                                              ║\n",
        "║  Test Set (held-out, {len(test_students)} students):                          ║\n",
        "║    AUC:      {test_auc:.4f}                                        ║\n",
        "║    Accuracy: {test_acc:.4f}                                        ║\n",
        "║                                                              ║\n",
        "║  Model: {sum(p.numel() for p in model_final.parameters()):,} parameters                              ║\n",
        "║  Training: {avg_epochs_cv} epochs (CV average)                           ║\n",
        "║  Split: Student-level (Split B), no leakage                  ║\n",
        "║                                                              ║\n",
        "╚══════════════════════════════════════════════════════════════╝\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9qu-TFRao-pp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8db9032f-293c-4ee6-90a4-3d752093307b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "DETAILED RESULTS FOR PAPER\n",
            "============================================================\n",
            "\n",
            "Table 1: Per-Fold Cross-Validation Results\n",
            "-------------------------------------------------------\n",
            "Fold   Students   Interactions   Val AUC    Val Acc    Epochs  \n",
            "-------------------------------------------------------\n",
            "1      91         173,698        0.7682     0.7501     13      \n",
            "2      91         122,617        0.7744     0.7451     16      \n",
            "3      91         132,709        0.7643     0.7538     12      \n",
            "4      90         117,003        0.7359     0.7248     14      \n",
            "5      90         133,058        0.7654     0.7562     16      \n",
            "-------------------------------------------------------\n",
            "Mean                             0.7617     0.7460     14.2    \n",
            "±Std                             0.0133     0.0112     1.6     \n",
            "\n",
            "\n",
            "Table 2: Model Comparison (for paper)\n",
            "-----------------------------------------------------------------\n",
            "Model                Val AUC          Test AUC     Params    \n",
            "-----------------------------------------------------------------\n",
            "GCNPure              0.7617 ± 0.0133   0.7255       16,193\n",
            "-----------------------------------------------------------------\n",
            "(Add rows for RGCN, Full GraphKT, xLSTMKT when available)\n",
            "\n",
            "\n",
            "Consistency Analysis:\n",
            "  AUC range across folds: 0.0385\n",
            "  ~ Moderately consistent (range < 0.05)\n",
            "\n",
            "  Coefficient of variation: 1.75%\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Cell 8: Paper-Ready Results Summary & Per-Fold Analysis\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"DETAILED RESULTS FOR PAPER\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Per-fold table\n",
        "print(\"\\nTable 1: Per-Fold Cross-Validation Results\")\n",
        "print(\"-\" * 55)\n",
        "print(f\"{'Fold':<6} {'Students':<10} {'Interactions':<14} {'Val AUC':<10} {'Val Acc':<10} {'Epochs':<8}\")\n",
        "print(\"-\" * 55)\n",
        "for i, r in enumerate(cv_results):\n",
        "    n_val_stu = len(fold_assignments[i]['val_students'])\n",
        "    df_val_f = df_non_test[df_non_test['student_id'].isin(fold_assignments[i]['val_students'])]\n",
        "    n_val_int = len(df_val_f)\n",
        "    print(f\"{i+1:<6} {n_val_stu:<10} {n_val_int:<14,} {r['val_auc']:<10.4f} {r['val_acc']:<10.4f} {r['stopped_epoch']:<8}\")\n",
        "\n",
        "print(\"-\" * 55)\n",
        "print(f\"{'Mean':<6} {'':10} {'':14} {np.mean(val_aucs):<10.4f} {np.mean(val_accs):<10.4f} {np.mean(stopped_epochs):<8.1f}\")\n",
        "print(f\"{'±Std':<6} {'':10} {'':14} {np.std(val_aucs):<10.4f} {np.std(val_accs):<10.4f} {np.std(stopped_epochs):<8.1f}\")\n",
        "\n",
        "# Summary table for paper\n",
        "print(f\"\\n\\nTable 2: Model Comparison (for paper)\")\n",
        "print(\"-\" * 65)\n",
        "print(f\"{'Model':<20} {'Val AUC':<16} {'Test AUC':<12} {'Params':<10}\")\n",
        "print(\"-\" * 65)\n",
        "print(f\"{'GCNPure':<20} {np.mean(val_aucs):.4f} ± {np.std(val_aucs):.4f}   {test_auc:<12.4f} {sum(p.numel() for p in model_final.parameters()):,}\")\n",
        "print(\"-\" * 65)\n",
        "print(\"(Add rows for RGCN, Full GraphKT, xLSTMKT when available)\")\n",
        "\n",
        "# Consistency check\n",
        "auc_range = np.max(val_aucs) - np.min(val_aucs)\n",
        "print(f\"\\n\\nConsistency Analysis:\")\n",
        "print(f\"  AUC range across folds: {auc_range:.4f}\")\n",
        "if auc_range < 0.03:\n",
        "    print(f\"  ✓ Highly consistent (range < 0.03)\")\n",
        "elif auc_range < 0.05:\n",
        "    print(f\"  ~ Moderately consistent (range < 0.05)\")\n",
        "else:\n",
        "    print(f\"  ⚠ High variance across folds - investigate fold differences\")\n",
        "\n",
        "print(f\"\\n  Coefficient of variation: {np.std(val_aucs)/np.mean(val_aucs)*100:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}